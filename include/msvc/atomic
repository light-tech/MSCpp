// atomic standard header
#pragma once
#ifndef _ATOMIC_
#define _ATOMIC_
#ifndef RC_INVOKED
#include <yvals.h>

#ifdef _M_CEE_PURE
#error <atomic> is not supported when compiling with /clr:pure.
#endif // _M_CEE_PURE

#include <limits.h>
#include <stddef.h>
#include <stdint.h>
#include <stdlib.h>
#include <type_traits>
#include <xatomic.h>

#pragma pack(push, _CRT_PACKING)
#pragma warning(push, _STL_WARNING_LEVEL)
#pragma warning(disable : _STL_DISABLED_WARNINGS)
_STL_DISABLE_CLANG_WARNINGS
#pragma push_macro("new")
#undef new

#pragma warning(disable : 4365) // signed / unsigned mismatch in implicit conversion
#pragma warning(disable : 4522) // multiple assignment operators specified

// LOCK-FREE PROPERTY
#define ATOMIC_BOOL_LOCK_FREE 2
#define ATOMIC_CHAR_LOCK_FREE 2
#define ATOMIC_CHAR16_T_LOCK_FREE 2
#define ATOMIC_CHAR32_T_LOCK_FREE 2
#define ATOMIC_WCHAR_T_LOCK_FREE 2
#define ATOMIC_SHORT_LOCK_FREE 2
#define ATOMIC_INT_LOCK_FREE 2
#define ATOMIC_LONG_LOCK_FREE 2
#define ATOMIC_LLONG_LOCK_FREE 2
#define ATOMIC_POINTER_LOCK_FREE 2

_STD_BEGIN
inline memory_order _Get_memory_order(memory_order _Order) { // get second memory_order argument for cas functions
                                                             // that take only one memory_order argument
    return _Order == memory_order_acq_rel ? memory_order_acquire
                                          : _Order == memory_order_release ? memory_order_relaxed : _Order;
}

// FUNCTION TEMPLATE kill_dependency
template <class _Ty>
_Ty kill_dependency(_Ty _Arg) noexcept { // magic template that kills dependency ordering when called
    return _Arg;
}

// GENERAL OPERATIONS ON ATOMIC TYPES (FORWARD DECLARATIONS)
template <class _Ty>
struct atomic;
template <class _Ty>
_NODISCARD bool atomic_is_lock_free(const volatile atomic<_Ty>*) noexcept;
template <class _Ty>
_NODISCARD bool atomic_is_lock_free(const atomic<_Ty>*) noexcept;
template <class _Ty>
void atomic_init(volatile atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
void atomic_init(atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
void atomic_store(volatile atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
void atomic_store(atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
void atomic_store_explicit(volatile atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
void atomic_store_explicit(atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_NODISCARD _Ty atomic_load(const volatile atomic<_Ty>*) noexcept;
template <class _Ty>
_NODISCARD _Ty atomic_load(const atomic<_Ty>*) noexcept;
template <class _Ty>
_NODISCARD _Ty atomic_load_explicit(const volatile atomic<_Ty>*, memory_order) noexcept;
template <class _Ty>
_NODISCARD _Ty atomic_load_explicit(const atomic<_Ty>*, memory_order) noexcept;
template <class _Ty>
_Ty atomic_exchange(volatile atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_exchange(atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_exchange_explicit(volatile atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_exchange_explicit(atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
bool atomic_compare_exchange_weak(volatile atomic<_Ty>*, _Ty*, _Ty) noexcept;
template <class _Ty>
bool atomic_compare_exchange_weak(atomic<_Ty>*, _Ty*, _Ty) noexcept;
template <class _Ty>
bool atomic_compare_exchange_weak_explicit(volatile atomic<_Ty>*, _Ty*, _Ty, memory_order, memory_order) noexcept;
template <class _Ty>
bool atomic_compare_exchange_weak_explicit(atomic<_Ty>*, _Ty*, _Ty, memory_order, memory_order) noexcept;
template <class _Ty>
bool atomic_compare_exchange_strong(volatile atomic<_Ty>*, _Ty*, _Ty) noexcept;
template <class _Ty>
bool atomic_compare_exchange_strong(atomic<_Ty>*, _Ty*, _Ty) noexcept;
template <class _Ty>
bool atomic_compare_exchange_strong_explicit(volatile atomic<_Ty>*, _Ty*, _Ty, memory_order, memory_order) noexcept;
template <class _Ty>
bool atomic_compare_exchange_strong_explicit(atomic<_Ty>*, _Ty*, _Ty, memory_order, memory_order) noexcept;

// TEMPLATED OPERATIONS ON ATOMIC TYPES (DECLARED BUT NOT DEFINED)
template <class _Ty>
_Ty atomic_fetch_add(volatile atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_add(atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_add_explicit(volatile atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_add_explicit(atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_sub(volatile atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_sub(atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_sub_explicit(volatile atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_sub_explicit(atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_and(volatile atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_and(atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_and_explicit(volatile atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_and_explicit(atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_or(volatile atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_or(atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_or_explicit(volatile atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_or_explicit(atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_xor(volatile atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_xor(atomic<_Ty>*, _Ty) noexcept;
template <class _Ty>
_Ty atomic_fetch_xor_explicit(volatile atomic<_Ty>*, _Ty, memory_order) noexcept;
template <class _Ty>
_Ty atomic_fetch_xor_explicit(atomic<_Ty>*, _Ty, memory_order) noexcept;

// STRUCT atomic_flag
#define ATOMIC_FLAG_INIT \
    { }
struct atomic_flag { // structure for managing flag with test-and-set semantics
    bool test_and_set(memory_order _Order = memory_order_seq_cst) volatile noexcept;
    bool test_and_set(memory_order _Order = memory_order_seq_cst) noexcept;
    void clear(memory_order _Order = memory_order_seq_cst) volatile noexcept;
    void clear(memory_order _Order = memory_order_seq_cst) noexcept;

    _Atomic_flag_t _My_flag;

    constexpr atomic_flag() noexcept : _My_flag{} {}
    atomic_flag(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) volatile = delete;
};

inline bool atomic_flag::test_and_set(
    memory_order _Order) volatile noexcept { // atomically set *this to true and return previous value
    return _Atomic_flag_test_and_set(&_My_flag, _Order) != 0;
}

inline bool atomic_flag::test_and_set(
    memory_order _Order) noexcept { // atomically set *this to true and return previous value
    return _Atomic_flag_test_and_set(&_My_flag, _Order) != 0;
}

inline void atomic_flag::clear(memory_order _Order) volatile noexcept { // atomically clear *this
    _Atomic_flag_clear(&_My_flag, _Order);
}

inline void atomic_flag::clear(memory_order _Order) noexcept { // atomically clear *this
    _Atomic_flag_clear(&_My_flag, _Order);
}

inline bool atomic_flag_test_and_set(
    volatile atomic_flag* _Flag) noexcept { // atomically set *_Flag to true and return previous value
    return _Atomic_flag_test_and_set(&_Flag->_My_flag, memory_order_seq_cst) != 0;
}

inline bool atomic_flag_test_and_set(
    atomic_flag* _Flag) noexcept { // atomically set *_Flag to true and return previous value
    return _Atomic_flag_test_and_set(&_Flag->_My_flag, memory_order_seq_cst) != 0;
}

inline bool atomic_flag_test_and_set_explicit(volatile atomic_flag* _Flag,
    memory_order _Order) noexcept { // atomically set *_Flag to true and return previous value
    return _Atomic_flag_test_and_set(&_Flag->_My_flag, _Order) != 0;
}

inline bool atomic_flag_test_and_set_explicit(
    atomic_flag* _Flag, memory_order _Order) noexcept { // atomically set *_Flag to true and return previous value
    return _Atomic_flag_test_and_set(&_Flag->_My_flag, _Order) != 0;
}

inline void atomic_flag_clear(volatile atomic_flag* _Flag) noexcept { // atomically clear *_Flag
    _Atomic_flag_clear(&_Flag->_My_flag, memory_order_seq_cst);
}

inline void atomic_flag_clear(atomic_flag* _Flag) noexcept { // atomically clear *_Flag
    _Atomic_flag_clear(&_Flag->_My_flag, memory_order_seq_cst);
}

inline void atomic_flag_clear_explicit(
    volatile atomic_flag* _Flag, memory_order _Order) noexcept { // atomically clear *_Flag
    _Atomic_flag_clear(&_Flag->_My_flag, _Order);
}

inline void atomic_flag_clear_explicit(atomic_flag* _Flag, memory_order _Order) noexcept { // atomically clear *_Flag
    _Atomic_flag_clear(&_Flag->_My_flag, _Order);
}

// STRUCT TEMPLATE _Atomic_impl
template <unsigned _Bytes>
struct _Atomic_impl { // struct for managing locks around operations on atomic types
    using _My_int = _Uint1_t; // "1 byte" means "no alignment required"

    constexpr _Atomic_impl() noexcept : _My_flag(0) { // default constructor
    }

    bool _Is_lock_free() const volatile { // operations that use locks are not lock-free
        return false;
    }

    void _Store(void* _Tgt, const void* _Src, memory_order _Order) volatile { // lock and store
        _Atomic_copy(&_My_flag, _Bytes, _Tgt, _Src, _Order);
    }

    void _Load(void* _Tgt, const void* _Src,
        memory_order _Order) const volatile { // lock and load
        _Atomic_copy(&_My_flag, _Bytes, _Tgt, _Src, _Order);
    }

    void _Exchange(void* _Left, void* _Right, memory_order _Order) volatile { // lock and exchange
        _Atomic_exchange(&_My_flag, _Bytes, _Left, _Right, _Order);
    }

    bool _Compare_exchange_weak(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // lock and compare/exchange
        return _Atomic_compare_exchange_weak(&_My_flag, _Bytes, _Tgt, _Exp, _Value, _Order1, _Order2) != 0;
    }

    bool _Compare_exchange_strong(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // lock and compare/exchange
        return _Atomic_compare_exchange_strong(&_My_flag, _Bytes, _Tgt, _Exp, _Value, _Order1, _Order2) != 0;
    }

private:
    mutable _Atomic_flag_t _My_flag;
};

// SPECIALIZATIONS OF _Atomic_impl

template <>
struct _Atomic_impl<1U> { // struct for managing lock-free operations on 1-byte atomic types
    using _My_int = _Uint1_t;

    bool _Is_lock_free() const volatile { // all operations are lock-free
        return true;
    }

    void _Store(void* _Tgt, const void* _Src, memory_order _Order) volatile { // store
        _Atomic_store_1((_My_int*) _Tgt, *(_My_int*) _Src, _Order);
    }

    void _Load(void* _Tgt, const void* _Src,
        memory_order _Order) const volatile { // load
        *(_My_int*) _Tgt = _Atomic_load_1((_My_int*) _Src, _Order);
    }

    void _Exchange(void* _Left, void* _Right, memory_order _Order) volatile { // exchange
        *(_My_int*) _Right = _Atomic_exchange_1((_My_int*) _Left, *(_My_int*) _Right, _Order);
    }

    bool _Compare_exchange_weak(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // compare/exchange
        return _Atomic_compare_exchange_weak_1((_My_int*) _Tgt, (_My_int*) _Exp, *(_My_int*) _Value, _Order1, _Order2)
               != 0;
    }

    bool _Compare_exchange_strong(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // compare/exchange
        return _Atomic_compare_exchange_strong_1(
            (_My_int*) _Tgt, (_My_int*) _Exp, *(_My_int*) _Value, _Order1, _Order2) != 0;
    }
};

template <>
struct _Atomic_impl<2U> { // struct for managing lock-free operations on 2-byte atomic types
    using _My_int = _Uint2_t;

    bool _Is_lock_free() const volatile { // all operations are lock-free
        return true;
    }

    void _Store(void* _Tgt, const void* _Src, memory_order _Order) volatile { // store
        _Atomic_store_2((_My_int*) _Tgt, *(_My_int*) _Src, _Order);
    }

    void _Load(void* _Tgt, const void* _Src,
        memory_order _Order) const volatile { // load
        *(_My_int*) _Tgt = _Atomic_load_2((_My_int*) _Src, _Order);
    }

    void _Exchange(void* _Left, void* _Right, memory_order _Order) volatile { // exchange
        *(_My_int*) _Right = _Atomic_exchange_2((_My_int*) _Left, *(_My_int*) _Right, _Order);
    }

    bool _Compare_exchange_weak(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // compare/exchange
        return _Atomic_compare_exchange_weak_2((_My_int*) _Tgt, (_My_int*) _Exp, *(_My_int*) _Value, _Order1, _Order2)
               != 0;
    }

    bool _Compare_exchange_strong(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // compare/exchange
        return _Atomic_compare_exchange_strong_2(
            (_My_int*) _Tgt, (_My_int*) _Exp, *(_My_int*) _Value, _Order1, _Order2) != 0;
    }
};

template <>
struct _Atomic_impl<4U> { // struct for managing lock-free operations on 4-byte atomic types
    using _My_int = _Uint4_t;

    bool _Is_lock_free() const volatile { // all operations are lock-free
        return true;
    }

    void _Store(void* _Tgt, const void* _Src, memory_order _Order) volatile { // store
        _Atomic_store_4((_My_int*) _Tgt, *(_My_int*) _Src, _Order);
    }

    void _Load(void* _Tgt, const void* _Src,
        memory_order _Order) const volatile { // load
        *(_My_int*) _Tgt = _Atomic_load_4((_My_int*) _Src, _Order);
    }

    void _Exchange(void* _Left, void* _Right, memory_order _Order) volatile { // exchange
        *(_My_int*) _Right = _Atomic_exchange_4((_My_int*) _Left, *(_My_int*) _Right, _Order);
    }

    bool _Compare_exchange_weak(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // compare/exchange
        return _Atomic_compare_exchange_weak_4((_My_int*) _Tgt, (_My_int*) _Exp, *(_My_int*) _Value, _Order1, _Order2)
               != 0;
    }

    bool _Compare_exchange_strong(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // compare/exchange
        return _Atomic_compare_exchange_strong_4(
            (_My_int*) _Tgt, (_My_int*) _Exp, *(_My_int*) _Value, _Order1, _Order2) != 0;
    }
};

template <>
struct _Atomic_impl<8U> { // struct for managing lock-free operations on 8-byte atomic types
    using _My_int = _Uint8_t;

    bool _Is_lock_free() const volatile { // all operations are lock-free
        return true;
    }

    void _Store(void* _Tgt, const void* _Src, memory_order _Order) volatile { // store
        _Atomic_store_8((_My_int*) _Tgt, *(_My_int*) _Src, _Order);
    }

    void _Load(void* _Tgt, const void* _Src,
        memory_order _Order) const volatile { // load
        *(_My_int*) _Tgt = _Atomic_load_8((_My_int*) _Src, _Order);
    }

    void _Exchange(void* _Left, void* _Right, memory_order _Order) volatile { // exchange
        *(_My_int*) _Right = _Atomic_exchange_8((_My_int*) _Left, *(_My_int*) _Right, _Order);
    }

    bool _Compare_exchange_weak(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // compare/exchange
        return _Atomic_compare_exchange_weak_8((_My_int*) _Tgt, (_My_int*) _Exp, *(_My_int*) _Value, _Order1, _Order2)
               != 0;
    }

    bool _Compare_exchange_strong(void* _Tgt, void* _Exp, const void* _Value, memory_order _Order1,
        memory_order _Order2) volatile { // compare/exchange
        return _Atomic_compare_exchange_strong_8(
            (_My_int*) _Tgt, (_My_int*) _Exp, *(_My_int*) _Value, _Order1, _Order2) != 0;
    }
};

// STRUCT TEMPLATE _Atomic_base
template <class _Ty,
    unsigned _Bytes>
struct _Atomic_base : _Atomic_impl<_Bytes> { // struct that defines most member functions of std::atomic
    using _Mybase = _Atomic_impl<_Bytes>;
    using _My_int = typename _Mybase::_My_int;

    constexpr _Atomic_base(_Ty _Val) noexcept
        : _My_val(_Val) { // construct from _Val, initialization is not an atomic operation
    }

#ifdef __clang__ // TRANSITION, VSO#406237
    constexpr _Atomic_base() _NOEXCEPT_COND(is_nothrow_default_constructible_v<_Ty>) : _My_val() {}
#else // ^^^ no workaround ^^^ // vvv workaround vvv
    _Atomic_base() = default;
#endif // VSO#406237

    _Atomic_base(const _Atomic_base&) = delete;
    _Atomic_base& operator=(const _Atomic_base&) = delete;
    _Atomic_base& operator=(const _Atomic_base&) volatile = delete;

    _Ty operator=(_Ty _Right) volatile noexcept { // assign from _Right
        this->_Store((void*) _STD addressof(_My_val), _STD addressof(_Right), memory_order_seq_cst);
        return _Right;
    }

    _Ty operator=(_Ty _Right) noexcept { // assign from _Right
        this->_Store(_STD addressof(_My_val), _STD addressof(_Right), memory_order_seq_cst);
        return _Right;
    }

    _NODISCARD bool is_lock_free() const volatile noexcept { // return true if operations are lock-free
        return this->_Is_lock_free();
    }

    _NODISCARD bool is_lock_free() const noexcept { // return true if operations are lock-free
        return this->_Is_lock_free();
    }

    void store(_Ty _Value, memory_order _Order = memory_order_seq_cst) volatile noexcept { // store _Value into *this
        this->_Store((void*) _STD addressof(_My_val), _STD addressof(_Value), _Order);
    }

    void store(_Ty _Value, memory_order _Order = memory_order_seq_cst) noexcept { // store _Value into *this
        this->_Store(_STD addressof(_My_val), _STD addressof(_Value), _Order);
    }

    _NODISCARD _Ty load(memory_order _Order = memory_order_seq_cst) const
        volatile noexcept { // return value stored in *this
        alignas(_Ty) unsigned char _Result[sizeof(_Ty)];
        this->_Load(&_Result, (const void*) _STD addressof(_My_val), _Order);
        return reinterpret_cast<_Ty&>(_Result);
    }

    _NODISCARD _Ty load(memory_order _Order = memory_order_seq_cst) const noexcept { // return value stored in *this
        alignas(_Ty) unsigned char _Result[sizeof(_Ty)];
        this->_Load(&_Result, _STD addressof(_My_val), _Order);
        return reinterpret_cast<_Ty&>(_Result);
    }

    operator _Ty() const volatile noexcept { // return value stored in *this
        return load();
    }

    operator _Ty() const noexcept { // return value stored in *this
        return load();
    }

    _Ty exchange(_Ty _Value,
        memory_order _Order = memory_order_seq_cst) volatile noexcept { // exchange value stored in *this with _Value
        this->_Exchange((void*) _STD addressof(_My_val), _STD addressof(_Value), _Order);
        return _Value;
    }

    _Ty exchange(
        _Ty _Value, memory_order _Order = memory_order_seq_cst) noexcept { // exchange value stored in *this with _Value
        this->_Exchange(_STD addressof(_My_val), _STD addressof(_Value), _Order);
        return _Value;
    }

    bool compare_exchange_weak(_Ty& _Exp, _Ty _Value, memory_order _Order1,
        memory_order _Order2) volatile noexcept { // compare and exchange value stored in *this with *_Exp, _Value
        return this->_Compare_exchange_weak(
            (void*) _STD addressof(_My_val), _STD addressof(_Exp), _STD addressof(_Value), _Order1, _Order2);
    }

    bool compare_exchange_weak(_Ty& _Exp, _Ty _Value, memory_order _Order1,
        memory_order _Order2) noexcept { // compare and exchange value stored in *this with *_Exp, _Value
        return this->_Compare_exchange_weak(
            _STD addressof(_My_val), _STD addressof(_Exp), _STD addressof(_Value), _Order1, _Order2);
    }

    bool compare_exchange_weak(_Ty& _Exp, _Ty _Value,
        memory_order _Order =
            memory_order_seq_cst) volatile noexcept { // compare and exchange value stored in *this with *_Exp, _Value
        return compare_exchange_weak(_Exp, _Value, _Order, _Get_memory_order(_Order));
    }

    bool compare_exchange_weak(_Ty& _Exp, _Ty _Value,
        memory_order _Order =
            memory_order_seq_cst) noexcept { // compare and exchange value stored in *this with *_Exp, _Value
        return compare_exchange_weak(_Exp, _Value, _Order, _Get_memory_order(_Order));
    }

    bool compare_exchange_strong(_Ty& _Exp, _Ty _Value, memory_order _Order1,
        memory_order _Order2) volatile noexcept { // compare and exchange value stored in *this with *_Exp, _Value
        return this->_Compare_exchange_strong(
            (void*) _STD addressof(_My_val), _STD addressof(_Exp), _STD addressof(_Value), _Order1, _Order2);
    }

    bool compare_exchange_strong(_Ty& _Exp, _Ty _Value, memory_order _Order1,
        memory_order _Order2) noexcept { // compare and exchange value stored in *this with *_Exp, _Value
        return this->_Compare_exchange_strong(
            _STD addressof(_My_val), _STD addressof(_Exp), _STD addressof(_Value), _Order1, _Order2);
    }

    bool compare_exchange_strong(_Ty& _Exp, _Ty _Value,
        memory_order _Order =
            memory_order_seq_cst) volatile noexcept { // compare and exchange value stored in *this with *_Exp, _Value
        return compare_exchange_strong(_Exp, _Value, _Order, _Get_memory_order(_Order));
    }

    bool compare_exchange_strong(_Ty& _Exp, _Ty _Value,
        memory_order _Order =
            memory_order_seq_cst) noexcept { // compare and exchange value stored in *this with *_Exp, _Value
        return compare_exchange_strong(_Exp, _Value, _Order, _Get_memory_order(_Order));
    }

#ifndef _ENABLE_ATOMIC_ALIGNMENT_FIX
    static_assert(alignof(_Ty) >= sizeof(_My_int),
        "You've instantiated std::atomic<T> with sizeof(T) equal to 2/4/8 and alignof(T) < sizeof(T). "
        "Before VS 2015 Update 2, this would have misbehaved at runtime. "
        "VS 2015 Update 2 was fixed to handle this correctly, "
        "but the fix inherently changes layout and breaks binary compatibility. "
        "Please define _ENABLE_ATOMIC_ALIGNMENT_FIX to acknowledge that you understand this, "
        "and that everything you're linking has been compiled with VS 2015 Update 2 (or later).");
#endif // _ENABLE_ATOMIC_ALIGNMENT_FIX

    alignas(sizeof(_My_int)) alignas(_Ty) _Ty _My_val;
};

// STRUCT TEMPLATE atomic
template <class _Ty>
struct atomic : _Atomic_base<_Ty, sizeof(_Ty)> { // template that manages values of _Ty atomically
    static_assert(is_trivially_copyable_v<_Ty>, "atomic<T> requires T to be trivially copyable.");

    using _My_base = _Atomic_base<_Ty, sizeof(_Ty)>;

    using value_type = _Ty;

#if _HAS_CXX17
    static constexpr bool is_always_lock_free =
        sizeof(_Ty) == 1 || sizeof(_Ty) == 2 || sizeof(_Ty) == 4 || sizeof(_Ty) == 8;

    static constexpr bool _Is_usually_lock_free = is_always_lock_free;
#endif // _HAS_CXX17

    atomic()              = default;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;

    constexpr atomic(_Ty _Val) noexcept : _My_base(_Val) { // construct from _Val
    }

    _Ty operator=(_Ty _Right) volatile noexcept { // assign from _Right
        return _My_base::operator=(_Right);
    }

    _Ty operator=(_Ty _Right) noexcept { // assign from _Right
        return _My_base::operator=(_Right);
    }
};
#define ATOMIC_VAR_INIT(_Val) \
    { _Val }
#define _ATOMIC_ITYPE _Atomic_bool
#define _ITYPE bool
#define _ISIZE 1
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_char
#define _ITYPE char
#define _ISIZE 1
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_schar
#define _ITYPE signed char
#define _ISIZE 1
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_uchar
#define _ITYPE unsigned char
#define _ISIZE 1
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_char16_t
#define _ITYPE char16_t
#define _ISIZE 2
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_char32_t
#define _ITYPE char32_t
#define _ISIZE 4
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_wchar_t
#define _ITYPE wchar_t
#define _ISIZE _WCHAR_T_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS

#ifndef _NATIVE_WCHAR_T_DEFINED
#define _ATOMIC_HAS_NO_SPECIALIZATION
#endif // _NATIVE_WCHAR_T_DEFINED

#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_short
#define _ITYPE short
#define _ISIZE _SHORT_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_ushort
#define _ITYPE unsigned short
#define _ISIZE _SHORT_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_int
#define _ITYPE int
#define _ISIZE _INT_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_uint
#define _ITYPE unsigned int
#define _ISIZE _INT_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_long
#define _ITYPE long
#define _ISIZE _LONG_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_ulong
#define _ITYPE unsigned long
#define _ISIZE _LONG_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_llong
#define _ITYPE long long
#define _ISIZE _LONGLONG_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_ullong
#define _ITYPE unsigned long long
#define _ISIZE _LONGLONG_SIZE
#define _ATOMIC_HAS_ARITHMETIC_OPS
#include <xxatomic>

#define _ATOMIC_ITYPE _Atomic_address
#define _ITYPE void*
#define _ISIZE _ADDR_SIZE
#define _ATOMIC_IS_ADDRESS_TYPE
#define _ATOMIC_HAS_NO_SPECIALIZATION
#include <xxatomic>

// GENERAL OPERATIONS ON ATOMIC TYPES
template <class _Ty>
_NODISCARD inline bool atomic_is_lock_free(const volatile atomic<_Ty>* _Atom) noexcept {
    return _Atom->is_lock_free();
}

template <class _Ty>
_NODISCARD inline bool atomic_is_lock_free(const atomic<_Ty>* _Atom) noexcept {
    return _Atom->is_lock_free();
}

template <class _Ty>
inline void atomic_init(volatile atomic<_Ty>* _Atom, _Ty _Value) noexcept {
    *_Atom = _Value;
}

template <class _Ty>
inline void atomic_init(atomic<_Ty>* _Atom, _Ty _Value) noexcept {
    *_Atom = _Value;
}

template <class _Ty>
inline void atomic_store(volatile atomic<_Ty>* _Atom, _Ty _Value) noexcept {
    _Atom->store(_Value);
}

template <class _Ty>
inline void atomic_store(atomic<_Ty>* _Atom, _Ty _Value) noexcept {
    _Atom->store(_Value);
}

template <class _Ty>
inline void atomic_store_explicit(volatile atomic<_Ty>* _Atom, _Ty _Value, memory_order _Order) noexcept {
    _Atom->store(_Value, _Order);
}

template <class _Ty>
inline void atomic_store_explicit(atomic<_Ty>* _Atom, _Ty _Value, memory_order _Order) noexcept {
    _Atom->store(_Value, _Order);
}

template <class _Ty>
_NODISCARD inline _Ty atomic_load(const volatile atomic<_Ty>* _Atom) noexcept {
    return _Atom->load();
}

template <class _Ty>
_NODISCARD inline _Ty atomic_load(const atomic<_Ty>* _Atom) noexcept {
    return _Atom->load();
}

template <class _Ty>
_NODISCARD inline _Ty atomic_load_explicit(const volatile atomic<_Ty>* _Atom, memory_order _Order) noexcept {
    return _Atom->load(_Order);
}

template <class _Ty>
_NODISCARD inline _Ty atomic_load_explicit(const atomic<_Ty>* _Atom, memory_order _Order) noexcept {
    return _Atom->load(_Order);
}

template <class _Ty>
inline _Ty atomic_exchange(volatile atomic<_Ty>* _Atom, _Ty _Value) noexcept {
    return _Atom->exchange(_Value);
}

template <class _Ty>
inline _Ty atomic_exchange(atomic<_Ty>* _Atom, _Ty _Value) noexcept {
    return _Atom->exchange(_Value);
}

template <class _Ty>
inline _Ty atomic_exchange_explicit(volatile atomic<_Ty>* _Atom, _Ty _Value, memory_order _Order) noexcept {
    return _Atom->exchange(_Value, _Order);
}

template <class _Ty>
inline _Ty atomic_exchange_explicit(atomic<_Ty>* _Atom, _Ty _Value, memory_order _Order) noexcept {
    return _Atom->exchange(_Value, _Order);
}

template <class _Ty>
inline bool atomic_compare_exchange_weak(volatile atomic<_Ty>* _Atom, _Ty* _Exp, _Ty _Value) noexcept {
    return _Atom->compare_exchange_weak(*_Exp, _Value);
}

template <class _Ty>
inline bool atomic_compare_exchange_weak(atomic<_Ty>* _Atom, _Ty* _Exp, _Ty _Value) noexcept {
    return _Atom->compare_exchange_weak(*_Exp, _Value);
}

template <class _Ty>
inline bool atomic_compare_exchange_weak_explicit(
    volatile atomic<_Ty>* _Atom, _Ty* _Exp, _Ty _Value, memory_order _Order1, memory_order _Order2) noexcept {
    return _Atom->compare_exchange_weak(*_Exp, _Value, _Order1, _Order2);
}

template <class _Ty>
inline bool atomic_compare_exchange_weak_explicit(
    atomic<_Ty>* _Atom, _Ty* _Exp, _Ty _Value, memory_order _Order1, memory_order _Order2) noexcept {
    return _Atom->compare_exchange_weak(*_Exp, _Value, _Order1, _Order2);
}

template <class _Ty>
inline bool atomic_compare_exchange_strong(volatile atomic<_Ty>* _Atom, _Ty* _Exp, _Ty _Value) noexcept {
    return _Atom->compare_exchange_strong(*_Exp, _Value);
}

template <class _Ty>
inline bool atomic_compare_exchange_strong(atomic<_Ty>* _Atom, _Ty* _Exp, _Ty _Value) noexcept {
    return _Atom->compare_exchange_strong(*_Exp, _Value);
}

template <class _Ty>
inline bool atomic_compare_exchange_strong_explicit(
    volatile atomic<_Ty>* _Atom, _Ty* _Exp, _Ty _Value, memory_order _Order1, memory_order _Order2) noexcept {
    return _Atom->compare_exchange_strong(*_Exp, _Value, _Order1, _Order2);
}

template <class _Ty>
inline bool atomic_compare_exchange_strong_explicit(
    atomic<_Ty>* _Atom, _Ty* _Exp, _Ty _Value, memory_order _Order1, memory_order _Order2) noexcept {
    return _Atom->compare_exchange_strong(*_Exp, _Value, _Order1, _Order2);
}

// OVERLOADS FOR POINTERS
template <class _Ty>
inline _Ty* atomic_fetch_add(volatile atomic<_Ty*>* _Atom, ptrdiff_t _Value) noexcept {
    return _Atom->fetch_add(_Value);
}

template <class _Ty>
inline _Ty* atomic_fetch_add(atomic<_Ty*>* _Atom, ptrdiff_t _Value) noexcept {
    return _Atom->fetch_add(_Value);
}

template <class _Ty>
inline _Ty* atomic_fetch_add_explicit(volatile atomic<_Ty*>* _Atom, ptrdiff_t _Value, memory_order _Order) noexcept {
    return _Atom->fetch_add(_Value, _Order);
}

template <class _Ty>
inline _Ty* atomic_fetch_add_explicit(atomic<_Ty*>* _Atom, ptrdiff_t _Value, memory_order _Order) noexcept {
    return _Atom->fetch_add(_Value, _Order);
}

template <class _Ty>
inline _Ty* atomic_fetch_sub(volatile atomic<_Ty*>* _Atom, ptrdiff_t _Value) noexcept {
    return _Atom->fetch_sub(_Value);
}

template <class _Ty>
inline _Ty* atomic_fetch_sub(atomic<_Ty*>* _Atom, ptrdiff_t _Value) noexcept {
    return _Atom->fetch_sub(_Value);
}

template <class _Ty>
inline _Ty* atomic_fetch_sub_explicit(volatile atomic<_Ty*>* _Atom, ptrdiff_t _Value, memory_order _Order) noexcept {
    return _Atom->fetch_sub(_Value, _Order);
}

template <class _Ty>
inline _Ty* atomic_fetch_sub_explicit(atomic<_Ty*>* _Atom, ptrdiff_t _Value, memory_order _Order) noexcept {
    return _Atom->fetch_sub(_Value, _Order);
}

// FENCES
extern "C" inline void atomic_thread_fence(
    memory_order _Order) noexcept { // force memory visibility and inhibit compiler reordering
    return _Atomic_thread_fence(_Order);
}

extern "C" inline void atomic_signal_fence(
    memory_order _Order) noexcept { // force memory visibility and inhibit compiler reordering
    return _Atomic_signal_fence(_Order);
}

// ATOMIC TYPEDEFS
using atomic_bool = atomic<bool>;

using atomic_char   = atomic<char>;
using atomic_schar  = atomic<signed char>;
using atomic_uchar  = atomic<unsigned char>;
using atomic_short  = atomic<short>;
using atomic_ushort = atomic<unsigned short>;
using atomic_int    = atomic<int>;
using atomic_uint   = atomic<unsigned int>;
using atomic_long   = atomic<long>;
using atomic_ulong  = atomic<unsigned long>;
using atomic_llong  = atomic<long long>;
using atomic_ullong = atomic<unsigned long long>;

using atomic_char16_t = atomic<char16_t>;
using atomic_char32_t = atomic<char32_t>;

using atomic_wchar_t = atomic<wchar_t>;

using atomic_int8_t   = atomic<int8_t>;
using atomic_uint8_t  = atomic<uint8_t>;
using atomic_int16_t  = atomic<int16_t>;
using atomic_uint16_t = atomic<uint16_t>;
using atomic_int32_t  = atomic<int32_t>;
using atomic_uint32_t = atomic<uint32_t>;
using atomic_int64_t  = atomic<int64_t>;
using atomic_uint64_t = atomic<uint64_t>;

using atomic_int_least8_t   = atomic<int_least8_t>;
using atomic_uint_least8_t  = atomic<uint_least8_t>;
using atomic_int_least16_t  = atomic<int_least16_t>;
using atomic_uint_least16_t = atomic<uint_least16_t>;
using atomic_int_least32_t  = atomic<int_least32_t>;
using atomic_uint_least32_t = atomic<uint_least32_t>;
using atomic_int_least64_t  = atomic<int_least64_t>;
using atomic_uint_least64_t = atomic<uint_least64_t>;

using atomic_int_fast8_t   = atomic<int_fast8_t>;
using atomic_uint_fast8_t  = atomic<uint_fast8_t>;
using atomic_int_fast16_t  = atomic<int_fast16_t>;
using atomic_uint_fast16_t = atomic<uint_fast16_t>;
using atomic_int_fast32_t  = atomic<int_fast32_t>;
using atomic_uint_fast32_t = atomic<uint_fast32_t>;
using atomic_int_fast64_t  = atomic<int_fast64_t>;
using atomic_uint_fast64_t = atomic<uint_fast64_t>;

using atomic_intptr_t  = atomic<intptr_t>;
using atomic_uintptr_t = atomic<uintptr_t>;
using atomic_size_t    = atomic<size_t>;
using atomic_ptrdiff_t = atomic<ptrdiff_t>;
using atomic_intmax_t  = atomic<intmax_t>;
using atomic_uintmax_t = atomic<uintmax_t>;
_STD_END
#pragma pop_macro("new")
_STL_RESTORE_CLANG_WARNINGS
#pragma warning(pop)
#pragma pack(pop)
#endif // RC_INVOKED
#endif // _ATOMIC_

/*
 * Copyright (c) by P.J. Plauger. All rights reserved.
 * Consult your license regarding permissions and restrictions.
V6.50:0009 */
